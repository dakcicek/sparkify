{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace - FE and Modelling Part\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import avg, stddev, split, udf, isnull, first, col, format_number, rand\n",
    "from pyspark.sql.types import IntegerType, ArrayType, FloatType, DoubleType, Row, DateType, StringType, LongType,TimestampType\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "import datetime\n",
    "from pyspark.sql.functions import from_utc_timestamp, from_unixtime\n",
    "\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "from pyspark.sql import functions as sF\n",
    "from pyspark.sql import types as sT\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import IntegerType, ArrayType, FloatType, DoubleType, Row, DateType\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier,DecisionTreeClassifier,LinearSVC\n",
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "def get_spark_session(master,appName):\n",
    "    spark = SparkSession.builder.master(master).appName(appName).getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    df_temp = spark.read.json(filename)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_phase_1(df_local):\n",
    "    df_local = df_local.withColumn('transaction_timestamp', from_unixtime(col('ts').cast(LongType())/1000).cast(TimestampType()))\n",
    "    df_local = df_local.withColumn('registration_timestamp', from_unixtime(col('registration').cast(LongType())/1000).cast(TimestampType()))\n",
    "    df_local = df_local.withColumn('year', F.col('transaction_timestamp').cast('string').substr(1, 4).cast('int')) \n",
    "    df_local = df_local.withColumn('month', F.col('transaction_timestamp').cast('string').substr(6, 2).cast('int'))\n",
    "    df_local = df_local.withColumn('day', F.col('transaction_timestamp').cast('string').substr(9, 2).cast('int'))\n",
    "    df_local = df_local.withColumn('location', split(col('location'),',').getItem(1))\n",
    "    df_local = df_local.withColumn('gender',F.when((col('gender')=='M'),1).otherwise(0))\n",
    "    #states = set([state[1].strip() for state in [x.split(',') for x in df_local.location.unique()]])\n",
    "    #states = set([state[1].strip() for state in [x.split(',') for x in np.array(df_local.select('location').distinct().toPandas()['location'].tolist())]])    \n",
    "    # Define a user defined function\n",
    "    #state = udf(lambda x: x.split(',')[1].strip())\n",
    "    #df_local = df_local.withColumn(\"location\", state(df.location))\n",
    "    df_local = df_local.filter(df_local.userId != \"\")\n",
    "    df_local = df_local.filter(col('userId').isNotNull())\n",
    "    df_local = df_local.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "    df_local = df_local.withColumn('churn_flag',F.when((col('page').isin(['Cancellation Confirmation','Cancel'])) |  (col('auth')=='Cancelled'),1 ).otherwise(0))\n",
    "    return df_local\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_phase_2(df_local):\n",
    "    ex = '\\(([^\\)]*)\\)'\n",
    "    userAgents = [x for x  in np.array(df_local.select('userAgent').distinct().toPandas()['userAgent'].tolist())] \n",
    "    mapping = {'Compatible': 1,  'Ipad': 2,  'Iphone': 3, 'Macintosh': 4,  'Windows nt 5.1': 5,  'Windows nt 6.0': 6,  'Windows nt 6.1': 7,  'Windows nt 6.2': 8,  'Windows nt 6.3': 9,  'X11': 10}\n",
    "    os_specific = udf(lambda x: mapping[re.findall(ex, x)[0].split(';')[0].capitalize()])\n",
    "    df_local = df_local.withColumn(\"os\", os_specific(df_local.userAgent).cast('int'))\n",
    "    df_local = df_local.withColumn('age', F.datediff(df.transaction_timestamp, df.registration_timestamp).alias('age').cast(IntegerType()))\n",
    "    df_local = df_local.drop('userAgent')\n",
    "    df_local = df_local.drop('registration')\n",
    "    for field in df_local.schema.fields:\n",
    "        if field.dataType==StringType():\n",
    "            df_local = df_local.withColumn(field.name, regexp_replace(field.name, '[^a-zA-Z0-9\\,\\-]', ''))\n",
    "    df_level = df_local.orderBy('ts', ascending=False).groupBy('userId').agg(first('level').alias('last_level'))\n",
    "    df_status = df_local.orderBy('ts', ascending=False).groupBy('userId').agg(first('status').cast('int').alias('last_status'))\n",
    "    df_local = df_local.join(df_level, on='userId') \n",
    "    df_local = df_local.join(df_status, on='userId') \n",
    "    df_local = df_local.drop('level')\n",
    "    df_local = df_local.drop('status')\n",
    "    df_local = df_local.drop('length')\n",
    "    return df_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_phase_3(df_new):\n",
    "    df_new = df_new.groupby('userId','year','month','day').agg(F.countDistinct(df_new.sessionId).alias('d_cnt_session'),\n",
    "                                                               F.max(df_new.location).cast('string').alias('d_location'),\n",
    "                                                               F.max(df_new.last_level).cast('string').alias('d_last_level'),\n",
    "                                                               F.max(df_new.last_status).cast('int').alias('d_last_status'),\n",
    "                                                               F.max(df_new.age).cast('int').alias('d_age'),\n",
    "                                                               F.max(df_new.os).cast('string').alias('d_os'),\n",
    "                                                               F.max(df_new.churn_flag).cast('int').alias('d_churn'),\n",
    "                                                               F.count(df_new.song).cast('int').alias('d_cnt_song'),\n",
    "                                                               F.max(df_new.itemInSession).alias('d_max_item_in_session'),\n",
    "                                                               F.min(df_new.itemInSession).alias('d_min_item_in_session'),\n",
    "                                                               F.avg(df_new.itemInSession).cast('int').alias('d_avg_item_in_session'),\n",
    "                                                               ((F.max(df_new.transaction_timestamp).cast(LongType()) - F.min(df_new.transaction_timestamp).cast(LongType()))/24*60).cast('double').alias('d_max_session_duration'),\n",
    "                                                               ((F.min(df_new.transaction_timestamp).cast(LongType()) - F.min(df_new.transaction_timestamp).cast(LongType()))/24*60).cast('double').alias('d_min_session_duration'),\n",
    "                                                               F.sum(F.when((df_new.page=='About'),1 ).otherwise(0)).alias('d_cnt_about'),\n",
    "                                                               F.sum(F.when((df_new.page=='AddFriend'),1 ).otherwise(0)).alias('d_cnt_addfriend'),\n",
    "                                                               F.sum(F.when((df_new.page=='AddtoPlaylist'),1 ).otherwise(0)).alias('d_cnt_addtoplaylist'),\n",
    "                                                               F.sum(F.when((df_new.page=='Downgrade'),1 ).otherwise(0)).alias('d_cnt_downgrade'),\n",
    "                                                               F.sum(F.when((df_new.page=='Error'),1 ).otherwise(0)).alias('d_cnt_error'),\n",
    "                                                               F.sum(F.when((df_new.page=='Help'),1 ).otherwise(0)).alias('d_cnt_help'),\n",
    "                                                               F.sum(F.when((df_new.page=='Home'),1 ).otherwise(0)).alias('d_cnt_home'),\n",
    "                                                               F.sum(F.when((df_new.page=='Logout'),1 ).otherwise(0)).alias('d_cnt_logout'),\n",
    "                                                               F.sum(F.when((df_new.page=='NextSong'),1 ).otherwise(0)).alias('d_cnt_nextsong'),\n",
    "                                                               F.sum(F.when((df_new.page=='RollAdvert'),1 ).otherwise(0)).alias('d_cnt_rolladvert'),\n",
    "                                                               F.sum(F.when((df_new.page=='SaveSettings'),1 ).otherwise(0)).alias('d_cnt_savesettings'),\n",
    "                                                               F.sum(F.when((df_new.page=='Settings'),1 ).otherwise(0)).alias('d_cnt_settings'),\n",
    "                                                               F.sum(F.when((df_new.page=='SubmitDowngrade'),1 ).otherwise(0)).alias('d_cnt_submitdowngrade'),\n",
    "                                                               F.sum(F.when((df_new.page=='SubmitUpgrade'),1 ).otherwise(0)).alias('d_cnt_submitupgrade'),\n",
    "                                                               F.sum(F.when((df_new.page=='ThumbsDown'),1 ).otherwise(0)).alias('d_cnt_thumbsdown'),\n",
    "                                                               F.sum(F.when((df_new.page=='ThumbsUp'),1 ).otherwise(0)).alias('d_cnt_thumbsup'),\n",
    "                                                               F.sum(F.when((df_new.page=='Upgrade'),1 ).otherwise(0)).alias('d_cnt_upgrade'),\n",
    "                                                               F.sum(F.when((df_new.song=='YoureTheOne'),1 ).otherwise(0)).alias('d_cnt_song_youretheone'),\n",
    "                                                               F.sum(F.when((df_new.song=='Revelry'),1 ).otherwise(0)).alias('d_cnt_song_revelry'),\n",
    "                                                               F.sum(F.when((df_new.song=='Undo'),1 ).otherwise(0)).alias('d_cnt_song_undo'),\n",
    "                                                               F.sum(F.when((df_new.song=='Sehrkosmisch'),1 ).otherwise(0)).alias('d_cnt_song_sehrkosmisch'),\n",
    "                                                               F.sum(F.when((df_new.song=='HornConcertoNo4inEflatK495IIRomanceAndantecantabile'),1 ).otherwise(0)).alias('d_cnt_song_hornconcerto'),\n",
    "                                                               F.sum(F.when((df_new.song=='DogDaysAreOverRadioEdit'),1 ).otherwise(0)).alias('d_cnt_song_dogdaysareoverradio'),\n",
    "                                                               F.sum(F.when((df_new.song=='UseSomebody'),1 ).otherwise(0)).alias('d_cnt_song_usesomebody'),\n",
    "                                                               F.sum(F.when((df_new.song=='Secrets'),1 ).otherwise(0)).alias('d_cnt_song_secrets'),\n",
    "                                                               F.sum(F.when((df_new.song=='Canada'),1 ).otherwise(0)).alias('d_cnt_song_canada'),\n",
    "                                                               F.sum(F.when((df_new.song=='SinceritEtJalousie'),1 ).otherwise(0)).alias('d_cnt_song_sinceritetjalousie'),\n",
    "                                                               F.sum(F.when((df_new.song=='AintMisbehavin'),1 ).otherwise(0)).alias('d_cnt_song_aintmisbehavin'),\n",
    "                                                               F.sum(F.when((df_new.song=='Reprsente'),1 ).otherwise(0)).alias('d_cnt_song_reprsente'),\n",
    "                                                               F.sum(F.when((df_new.song=='LoveStory'),1 ).otherwise(0)).alias('d_cnt_song_lovestory'),\n",
    "                                                               F.sum(F.when((df_new.song=='Fireflies'),1 ).otherwise(0)).alias('d_cnt_song_fireflies'),\n",
    "                                                               F.sum(F.when((df_new.song=='CatchYouBabyStevePitronMaxSannaRadioEdit'),1 ).otherwise(0)).alias('d_cnt_song_catchyoubabysteve'),\n",
    "                                                               F.sum(F.when((df_new.song=='HeySoulSister'),1 ).otherwise(0)).alias('d_cnt_song_heysoulsister'),\n",
    "                                                               F.sum(F.when((df_new.song=='TheGift'),1 ).otherwise(0)).alias('d_cnt_song_thegift'),\n",
    "                                                               F.sum(F.when((df_new.song=='Invalid'),1 ).otherwise(0)).alias('d_cnt_song_invalid'),\n",
    "                                                               F.sum(F.when((df_new.song=='SomebodyToLove'),1 ).otherwise(0)).alias('d_cnt_song_somebodytolove'),\n",
    "                                                               F.sum(F.when((df_new.artist=='KingsOfLeon'),1 ).otherwise(0)).alias('d_cnt_kingsofleon'),\n",
    "                                                               F.sum(F.when((df_new.artist=='Coldplay'),1 ).otherwise(0)).alias('d_cnt_coldplay'),\n",
    "                                                               F.sum(F.when((df_new.artist=='DwightYoakam'),1 ).otherwise(0)).alias('d_cnt_dwightyoakam'),\n",
    "                                                               F.sum(F.when((df_new.artist=='FlorenceTheMachine'),1 ).otherwise(0)).alias('d_cnt_florencethemachine'),\n",
    "                                                               F.sum(F.when((df_new.artist=='TheBlackKeys'),1 ).otherwise(0)).alias('d_cnt_theblackkeys'),\n",
    "                                                               F.sum(F.when((df_new.artist=='Bjrk'),1 ).otherwise(0)).alias('d_cnt_bjrk'),\n",
    "                                                               F.sum(F.when((df_new.artist=='JustinBieber'),1 ).otherwise(0)).alias('d_cnt_justinbieber'),\n",
    "                                                               F.sum(F.when((df_new.artist=='JackJohnson'),1 ).otherwise(0)).alias('d_cnt_jackjohnson'),\n",
    "                                                               F.sum(F.when((df_new.artist=='TaylorSwift'),1 ).otherwise(0)).alias('d_cnt_taylorswift'),\n",
    "                                                               F.sum(F.when((df_new.artist=='Harmonia'),1 ).otherwise(0)).alias('d_cnt_harmonia'),\n",
    "                                                               F.sum(F.when((df_new.artist=='AllianceEthnik'),1 ).otherwise(0)).alias('d_cnt_allianceethnik'),\n",
    "                                                               F.sum(F.when((df_new.artist=='GunsNRoses'),1 ).otherwise(0)).alias('d_cnt_gunsnroses'),\n",
    "                                                               F.sum(F.when((df_new.artist=='Train'),1 ).otherwise(0)).alias('d_cnt_train'),\n",
    "                                                               F.sum(F.when((df_new.artist=='Eminem'),1 ).otherwise(0)).alias('d_cnt_eminem'),\n",
    "                                                               F.sum(F.when((df_new.artist=='OneRepublic'),1 ).otherwise(0)).alias('d_cnt_onerepublic')\n",
    "                                                              )\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_phase_4(df_new_local):\n",
    "    df_new_local = df_new_local.groupby('userId').agg(   F.max(df_new_local.d_churn).cast('int').alias('label'),                                                         \n",
    "                                                         F.max(df_new_local.d_age).cast('int').alias('age'),\n",
    "                                                         F.max(df_new_local.d_os).cast('int').alias('os'),\n",
    "                                                         F.max(df_new_local.d_location).cast('string').alias('location'),       \n",
    "                                                         F.max(df_new_local.d_last_level).cast('string').alias('2m_last_level'),\n",
    "                                                         F.max(df_new_local.d_last_status).cast('int').alias('2m_last_status'),\n",
    "                                                         F.sum(df_new_local.d_cnt_song).alias('2m_avg_song'),   \n",
    "                                                         F.sum(df_new_local.d_cnt_session).alias('2m_sum_session'),\n",
    "                                                         F.avg(df_new_local.d_cnt_session).alias('2m_avg_session'),\n",
    "                                                         F.max(df_new_local.d_cnt_session).alias('2m_max_session'),\n",
    "                                                         F.max(df_new_local.d_max_item_in_session).alias('2m_max_item_in_session'),\n",
    "                                                         F.min(df_new_local.d_min_item_in_session).alias('2m_min_item_in_session'),\n",
    "                                                         F.avg(df_new_local.d_avg_item_in_session).cast('int').alias('2m_avg_item_in_session'),\n",
    "                                                         F.max(df_new_local.d_max_session_duration).cast('int').alias('2m_max_session_duration'),\n",
    "                                                         F.max(df_new_local.d_min_session_duration).cast('int').alias('2m_min_session_duration'),\n",
    "                                                         F.avg(df_new_local.d_cnt_rolladvert).alias('2m_avg_page_rolladvert'),\n",
    "                                                         F.avg(df_new_local.d_cnt_settings).alias('2m_avg_page_settings'),\n",
    "                                                         F.avg(df_new_local.d_cnt_downgrade).alias('2m_avg_page_downgrade'),\n",
    "                                                         F.avg(df_new_local.d_cnt_nextsong).alias('2m_avg_page_nextsong'),\n",
    "                                                         F.avg(df_new_local.d_cnt_error).alias('2m_avg_page_error'),\n",
    "                                                         F.avg(df_new_local.d_cnt_about).alias('2m_avg_page_about'),\n",
    "                                                         F.avg(df_new_local.d_cnt_upgrade).alias('2m_avg_page_upgrade'),\n",
    "                                                         F.avg(df_new_local.d_cnt_home).alias('2m_avg_page_home'),\n",
    "                                                         F.avg(df_new_local.d_cnt_logout).alias('2m_avg_page_logout'),\n",
    "                                                         F.avg(df_new_local.d_cnt_addtoplaylist).alias('2m_avg_page_addtoplaylist'),\n",
    "                                                         F.avg(df_new_local.d_cnt_thumbsdown).alias('2m_avg_page_thumbsdown'),\n",
    "                                                         F.avg(df_new_local.d_cnt_thumbsup).alias('2m_avg_page_thumbsup'),\n",
    "                                                         F.avg(df_new_local.d_cnt_savesettings).alias('2m_avg_page_savesettings'),\n",
    "                                                         F.avg(df_new_local.d_cnt_addfriend).alias('2m_avg_page_addfriend'),\n",
    "                                                         F.avg(df_new_local.d_cnt_submitupgrade).alias('2m_avg_page_submitupgrade'),\n",
    "                                                         F.avg(df_new_local.d_cnt_help).alias('2m_avg_page_help'),\n",
    "                                                         F.avg(df_new_local.d_cnt_submitdowngrade).alias('2m_avg_page_submitdowngrade'),\n",
    "                                                         F.sum(df_new_local.d_cnt_rolladvert).alias('2m_sum_page_rolladvert'),\n",
    "                                                         F.sum(df_new_local.d_cnt_settings).alias('2m_sum_page_settings'),\n",
    "                                                         F.sum(df_new_local.d_cnt_downgrade).alias('2m_sum_page_downgrade'),\n",
    "                                                         F.sum(df_new_local.d_cnt_nextsong).alias('2m_sum_page_nextsong'),\n",
    "                                                         F.sum(df_new_local.d_cnt_error).alias('2m_sum_page_error'),\n",
    "                                                         F.sum(df_new_local.d_cnt_about).alias('2m_sum_page_about'),\n",
    "                                                         F.sum(df_new_local.d_cnt_upgrade).alias('2m_sum_page_upgrade'),\n",
    "                                                         F.sum(df_new_local.d_cnt_home).alias('2m_sum_page_home'),\n",
    "                                                         F.sum(df_new_local.d_cnt_logout).alias('2m_sum_page_logout'),\n",
    "                                                         F.sum(df_new_local.d_cnt_addtoplaylist).alias('2m_sum_page_addtoplaylist'),\n",
    "                                                         F.sum(df_new_local.d_cnt_thumbsdown).alias('2m_sum_page_thumbsdown'),\n",
    "                                                         F.sum(df_new_local.d_cnt_thumbsup).alias('2m_sum_page_thumbsup'),\n",
    "                                                         F.sum(df_new_local.d_cnt_savesettings).alias('2m_sum_page_savesettings'),\n",
    "                                                         F.sum(df_new_local.d_cnt_addfriend).alias('2m_sum_page_addfriend'),\n",
    "                                                         F.sum(df_new_local.d_cnt_submitupgrade).alias('2m_sum_page_submitupgrade'),\n",
    "                                                         F.sum(df_new_local.d_cnt_help).alias('2m_sum_page_help'),\n",
    "                                                         F.sum(df_new_local.d_cnt_submitdowngrade).alias('2m_sum_page_submitdowngrade'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_youretheone).alias('2m_avg_song_youretheone'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_revelry).alias('2m_avg_song_revelry'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_undo).alias('2m_avg_song_undo'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_sehrkosmisch).alias('2m_avg_song_sehrkosmisch'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_hornconcerto).alias('2m_avg_song_hornconcerto'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_dogdaysareoverradio).alias('2m_avg_song_dogdaysareoverradio'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_usesomebody).alias('2m_avg_song_usesomebody'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_secrets).alias('2m_avg_song_secrets'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_canada).alias('2m_avg_song_canada'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_sinceritetjalousie).alias('2m_avg_song_sinceritetjalousie'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_aintmisbehavin).alias('2m_avg_song_aintmisbehavin'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_reprsente).alias('2m_avg_song_reprsente'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_lovestory).alias('2m_avg_song_lovestory'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_fireflies).alias('2m_avg_song_fireflies'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_catchyoubabysteve).alias('2m_avg_song_catchyoubabysteve'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_heysoulsister).alias('2m_avg_song_heysoulsister'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_thegift).alias('2m_avg_song_thegift'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_invalid).alias('2m_avg_song_invalid'),\n",
    "                                                         F.avg(df_new_local.d_cnt_song_somebodytolove).alias('2m_avg_song_somebodytolove'),\n",
    "                                                         F.avg(df_new_local.d_cnt_kingsofleon).alias('2m_avg_artist_kingsofleon'),\n",
    "                                                         F.avg(df_new_local.d_cnt_coldplay).alias('2m_avg_artist_coldplay'),\n",
    "                                                         F.avg(df_new_local.d_cnt_dwightyoakam).alias('2m_avg_artist_dwightyoakam'),\n",
    "                                                         F.avg(df_new_local.d_cnt_florencethemachine).alias('2m_avg_artist_florencethemachine'),\n",
    "                                                         F.avg(df_new_local.d_cnt_theblackkeys).alias('2m_avg_artist_theblackkeys'),\n",
    "                                                         F.avg(df_new_local.d_cnt_bjrk).alias('2m_avg_artist_bjrk'),\n",
    "                                                         F.avg(df_new_local.d_cnt_justinbieber).alias('2m_avg_artist_justinbieber'),\n",
    "                                                         F.avg(df_new_local.d_cnt_jackjohnson).alias('2m_avg_artist_jackjohnson'),\n",
    "                                                         F.avg(df_new_local.d_cnt_taylorswift).alias('2m_avg_artist_taylorswift'),\n",
    "                                                         F.avg(df_new_local.d_cnt_harmonia).alias('2m_avg_artist_harmonia'),\n",
    "                                                         F.avg(df_new_local.d_cnt_allianceethnik).alias('2m_avg_artist_allianceethnik'),\n",
    "                                                         F.avg(df_new_local.d_cnt_gunsnroses).alias('2m_avg_artist_gunsnroses'),\n",
    "                                                         F.avg(df_new_local.d_cnt_train).alias('2m_avg_artist_train'),\n",
    "                                                         F.avg(df_new_local.d_cnt_eminem).alias('2m_avg_artist_eminem'),\n",
    "                                                         F.avg(df_new_local.d_cnt_onerepublic).alias('2m_avg_artist_onerepublic')\n",
    "                                                                  )\n",
    "    df_new_local = df_new_local.drop('userId')\n",
    "    return df_new_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(\"local\",\"Udacity - Sparkify\")\n",
    "df=load_dataset('mini_sparkify_event_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning 286500\n",
      "after cleaning 278154\n"
     ]
    }
   ],
   "source": [
    "print('before cleaning {}'.format(df.count()))\n",
    "df = feature_engineering_phase_1(df)\n",
    "print('after cleaning {}'.format(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I cleaned the dataset, transformed some features to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering_phase_2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = feature_engineering_phase_3(df)\n",
    "#df_dap = df.toPandas()\n",
    "#df_dap.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering_phase_4(df)\n",
    "#df_map = df.toPandas()\n",
    "#df_map.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- os: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- 2m_last_level: string (nullable = true)\n",
      " |-- 2m_last_status: integer (nullable = true)\n",
      " |-- 2m_avg_song: long (nullable = true)\n",
      " |-- 2m_sum_session: long (nullable = true)\n",
      " |-- 2m_avg_session: double (nullable = true)\n",
      " |-- 2m_max_session: long (nullable = true)\n",
      " |-- 2m_max_item_in_session: long (nullable = true)\n",
      " |-- 2m_min_item_in_session: long (nullable = true)\n",
      " |-- 2m_avg_item_in_session: integer (nullable = true)\n",
      " |-- 2m_max_session_duration: integer (nullable = true)\n",
      " |-- 2m_min_session_duration: integer (nullable = true)\n",
      " |-- 2m_avg_page_rolladvert: double (nullable = true)\n",
      " |-- 2m_avg_page_settings: double (nullable = true)\n",
      " |-- 2m_avg_page_downgrade: double (nullable = true)\n",
      " |-- 2m_avg_page_nextsong: double (nullable = true)\n",
      " |-- 2m_avg_page_error: double (nullable = true)\n",
      " |-- 2m_avg_page_about: double (nullable = true)\n",
      " |-- 2m_avg_page_upgrade: double (nullable = true)\n",
      " |-- 2m_avg_page_home: double (nullable = true)\n",
      " |-- 2m_avg_page_logout: double (nullable = true)\n",
      " |-- 2m_avg_page_addtoplaylist: double (nullable = true)\n",
      " |-- 2m_avg_page_thumbsdown: double (nullable = true)\n",
      " |-- 2m_avg_page_thumbsup: double (nullable = true)\n",
      " |-- 2m_avg_page_savesettings: double (nullable = true)\n",
      " |-- 2m_avg_page_addfriend: double (nullable = true)\n",
      " |-- 2m_avg_page_submitupgrade: double (nullable = true)\n",
      " |-- 2m_avg_page_help: double (nullable = true)\n",
      " |-- 2m_avg_page_submitdowngrade: double (nullable = true)\n",
      " |-- 2m_sum_page_rolladvert: long (nullable = true)\n",
      " |-- 2m_sum_page_settings: long (nullable = true)\n",
      " |-- 2m_sum_page_downgrade: long (nullable = true)\n",
      " |-- 2m_sum_page_nextsong: long (nullable = true)\n",
      " |-- 2m_sum_page_error: long (nullable = true)\n",
      " |-- 2m_sum_page_about: long (nullable = true)\n",
      " |-- 2m_sum_page_upgrade: long (nullable = true)\n",
      " |-- 2m_sum_page_home: long (nullable = true)\n",
      " |-- 2m_sum_page_logout: long (nullable = true)\n",
      " |-- 2m_sum_page_addtoplaylist: long (nullable = true)\n",
      " |-- 2m_sum_page_thumbsdown: long (nullable = true)\n",
      " |-- 2m_sum_page_thumbsup: long (nullable = true)\n",
      " |-- 2m_sum_page_savesettings: long (nullable = true)\n",
      " |-- 2m_sum_page_addfriend: long (nullable = true)\n",
      " |-- 2m_sum_page_submitupgrade: long (nullable = true)\n",
      " |-- 2m_sum_page_help: long (nullable = true)\n",
      " |-- 2m_sum_page_submitdowngrade: long (nullable = true)\n",
      " |-- 2m_avg_song_youretheone: double (nullable = true)\n",
      " |-- 2m_avg_song_revelry: double (nullable = true)\n",
      " |-- 2m_avg_song_undo: double (nullable = true)\n",
      " |-- 2m_avg_song_sehrkosmisch: double (nullable = true)\n",
      " |-- 2m_avg_song_hornconcerto: double (nullable = true)\n",
      " |-- 2m_avg_song_dogdaysareoverradio: double (nullable = true)\n",
      " |-- 2m_avg_song_usesomebody: double (nullable = true)\n",
      " |-- 2m_avg_song_secrets: double (nullable = true)\n",
      " |-- 2m_avg_song_canada: double (nullable = true)\n",
      " |-- 2m_avg_song_sinceritetjalousie: double (nullable = true)\n",
      " |-- 2m_avg_song_aintmisbehavin: double (nullable = true)\n",
      " |-- 2m_avg_song_reprsente: double (nullable = true)\n",
      " |-- 2m_avg_song_lovestory: double (nullable = true)\n",
      " |-- 2m_avg_song_fireflies: double (nullable = true)\n",
      " |-- 2m_avg_song_catchyoubabysteve: double (nullable = true)\n",
      " |-- 2m_avg_song_heysoulsister: double (nullable = true)\n",
      " |-- 2m_avg_song_thegift: double (nullable = true)\n",
      " |-- 2m_avg_song_invalid: double (nullable = true)\n",
      " |-- 2m_avg_song_somebodytolove: double (nullable = true)\n",
      " |-- 2m_avg_artist_kingsofleon: double (nullable = true)\n",
      " |-- 2m_avg_artist_coldplay: double (nullable = true)\n",
      " |-- 2m_avg_artist_dwightyoakam: double (nullable = true)\n",
      " |-- 2m_avg_artist_florencethemachine: double (nullable = true)\n",
      " |-- 2m_avg_artist_theblackkeys: double (nullable = true)\n",
      " |-- 2m_avg_artist_bjrk: double (nullable = true)\n",
      " |-- 2m_avg_artist_justinbieber: double (nullable = true)\n",
      " |-- 2m_avg_artist_jackjohnson: double (nullable = true)\n",
      " |-- 2m_avg_artist_taylorswift: double (nullable = true)\n",
      " |-- 2m_avg_artist_harmonia: double (nullable = true)\n",
      " |-- 2m_avg_artist_allianceethnik: double (nullable = true)\n",
      " |-- 2m_avg_artist_gunsnroses: double (nullable = true)\n",
      " |-- 2m_avg_artist_train: double (nullable = true)\n",
      " |-- 2m_avg_artist_eminem: double (nullable = true)\n",
      " |-- 2m_avg_artist_onerepublic: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age','os','2m_last_status','2m_avg_song','2m_sum_session','2m_avg_session','2m_max_session','2m_max_item_in_session',\\\n",
    "                '2m_min_item_in_session','2m_avg_item_in_session','2m_max_session_duration','2m_min_session_duration','2m_avg_page_rolladvert',\\\n",
    "                '2m_avg_page_settings','2m_avg_page_downgrade','2m_avg_page_nextsong','2m_avg_page_error','2m_avg_page_about','2m_avg_page_upgrade',\\\n",
    "                '2m_avg_page_home','2m_avg_page_logout','2m_avg_page_addtoplaylist','2m_avg_page_thumbsdown','2m_avg_page_thumbsup','2m_avg_page_savesettings',\\\n",
    "                '2m_avg_page_addfriend','2m_avg_page_submitupgrade','2m_avg_page_help','2m_avg_page_submitdowngrade','2m_sum_page_rolladvert','2m_sum_page_settings',\\\n",
    "                '2m_sum_page_downgrade','2m_sum_page_nextsong','2m_sum_page_error','2m_sum_page_about','2m_sum_page_upgrade','2m_sum_page_home','2m_sum_page_logout',\\\n",
    "                '2m_sum_page_addtoplaylist','2m_sum_page_thumbsdown','2m_sum_page_thumbsup','2m_sum_page_savesettings','2m_sum_page_addfriend','2m_sum_page_submitupgrade',\\\n",
    "                '2m_sum_page_help','2m_sum_page_submitdowngrade','2m_avg_song_youretheone','2m_avg_song_revelry','2m_avg_song_undo','2m_avg_song_sehrkosmisch',\\\n",
    "                '2m_avg_song_hornconcerto','2m_avg_song_dogdaysareoverradio','2m_avg_song_usesomebody','2m_avg_song_secrets','2m_avg_song_canada','2m_avg_song_sinceritetjalousie',\\\n",
    "                '2m_avg_song_aintmisbehavin','2m_avg_song_reprsente','2m_avg_song_lovestory','2m_avg_song_fireflies','2m_avg_song_catchyoubabysteve','2m_avg_song_heysoulsister',\\\n",
    "                '2m_avg_song_thegift','2m_avg_song_invalid','2m_avg_song_somebodytolove','2m_avg_artist_kingsofleon','2m_avg_artist_coldplay','2m_avg_artist_dwightyoakam',\\\n",
    "                '2m_avg_artist_florencethemachine','2m_avg_artist_theblackkeys','2m_avg_artist_bjrk','2m_avg_artist_justinbieber','2m_avg_artist_jackjohnson',\\\n",
    "                '2m_avg_artist_taylorswift','2m_avg_artist_harmonia','2m_avg_artist_allianceethnik','2m_avg_artist_gunsnroses','2m_avg_artist_train',\\\n",
    "                '2m_avg_artist_eminem','2m_avg_artist_onerepublic']\n",
    "\n",
    "indexer_location = StringIndexer(inputCol='location', outputCol='location_index')\n",
    "indexer_2m_last_level = StringIndexer(inputCol='2m_last_level', outputCol='level_index')\n",
    "assembler = VectorAssembler(inputCols=numeric_features, outputCol='features')\n",
    "process_pipeline = Pipeline(stages=[indexer_location, indexer_2m_last_level, assembler])\n",
    "modelling_dataframe = process_pipeline.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = modelling_dataframe.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_the_model(classifier):\n",
    "    \"\"\"\n",
    "    fit and predict with training and test data\n",
    "    :param classifier : Classifier Algorithm class\n",
    "    :return classifer and prediction results\n",
    "    \"\"\"\n",
    "    clf = classifier.fit(train)\n",
    "    result = clf.transform(test)\n",
    "    return clf, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(result):\n",
    "    \"\"\"\n",
    "    :param result : prediction results which will be use to print metrics\n",
    "    \"\"\"\n",
    "    evaluator= MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "    print('Accuracy: {}'.format(evaluator.evaluate(result.select('label','prediction'), {evaluator.metricName: \"accuracy\"})))\n",
    "    print('F1 Score:{}'.format(evaluator.evaluate(result.select('label','prediction'), {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importance(clf, cols):\n",
    "    a = {}\n",
    "    feat_imp = clf.featureImportances\n",
    "    for i in range(len(cols)):\n",
    "        a.update({cols[i]:feat_imp[i]})\n",
    "        \n",
    "    feat_importance = pd.DataFrame.from_dict(a, orient=\"index\").reset_index()\n",
    "    feat_importance.columns = ['feature', 'importance']\n",
    "    feat_importance_top_20 = feat_importance.sort_values(by=\"importance\", ascending=False).head(20)\n",
    "    return feat_importance_top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_pipeline(model_algorithm):\n",
    "    \"\"\"\n",
    "    :param model_algorithm : classifier algorithm class\n",
    "    :return classifier, prediction result\n",
    "    \"\"\"\n",
    "    clf, result = fit_the_model(model_algorithm)\n",
    "    print_metrics(result)    \n",
    "    return clf,result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model with randomforest with default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7058823529411765\n",
      "F1 Score:0.6288515406162466\n",
      "Test F1 Score : 62.89%\n"
     ]
    }
   ],
   "source": [
    "x,y=run_model_pipeline(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model with GradientBoosting with default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7058823529411765\n",
      "F1 Score:0.6591970121381886\n",
      "Test F1 Score : 65.92%\n"
     ]
    }
   ],
   "source": [
    "x,y=run_model_pipeline(GBTClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model with LogisticRegression with default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7058823529411765\n",
      "F1 Score:0.6954248366013073\n",
      "Test F1 Score : 69.54%\n"
     ]
    }
   ],
   "source": [
    "x,y=run_model_pipeline(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to default parameters, LogisticRegression is more good than the others based on f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_location = StringIndexer(inputCol='location', outputCol='location_index')\n",
    "indexer_2m_last_level = StringIndexer(inputCol='2m_last_level', outputCol='level_index')\n",
    "assembler = VectorAssembler(inputCols=numeric_features, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "indexer_location = StringIndexer(inputCol='location', outputCol='location_index')\n",
    "indexer_2m_last_level = StringIndexer(inputCol='2m_last_level', outputCol='level_index')\n",
    "assembler = VectorAssembler(inputCols=numeric_features, outputCol='features')\n",
    "pipeline_tune_lr = Pipeline(stages=[indexer_location, indexer_2m_last_level, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrees=[20,60]\n",
    "maxDepth=[10,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(classifier_model_alg, numTrees_param, maxDepth_param, metricName_param, numFolds_param ):\n",
    "    paramGrid = ParamGridBuilder().addGrid(classifier_model_alg.numTrees, numTrees_param).addGrid(classifier_model_alg.maxDepth, maxDepth_param).build() - \n",
    "    crossval = CrossValidator(estimator = Pipeline(stages=[classifier_model_alg]), estimatorParamMaps = paramGrid, evaluator = MulticlassClassificationEvaluator(metricName=metricName_param),numFolds = numFolds_param)\n",
    "    cross_validation_model = crossval.fit(train)\n",
    "    prediction_results = cross_validation_model.transform(test)\n",
    "    return prediction_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the randomforest alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tune_model(RandomForestClassifier(), [20,60], [10,30], 'f1', 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7647058823529411\n",
      "F1 Score:0.7030812324929971\n",
      "Test F1 Score : 70.31%\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the randomforest alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = tune_model(RandomForestClassifier(), [20,70], [5,30], 'f1', 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7352941176470589\n",
      "F1 Score:0.6479031804109204\n",
      "Test F1 Score : 64.79%\n"
     ]
    }
   ],
   "source": [
    "print_metrics(pred_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = tune_model(RandomForestClassifier(), [20,80], [10,25], 'f1', 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7058823529411765\n",
      "F1 Score:0.5841784989858012\n",
      "Test F1 Score : 58.42%\n"
     ]
    }
   ],
   "source": [
    "print_metrics(pred_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the randomforest alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tune_model(RandomForestClassifier(), [20,75], [10,30], 'f1', 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7647058823529411\n",
      "F1 Score:0.7030812324929971\n",
      "Test F1 Score : 70.31%\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='label', outputCol='label')\n",
    "assembler = VectorAssembler(inputCols=numeric_features, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_lr(classifier_model_alg, maxIter_param,regParam_param, elasticNetParam_param, numFolds_param ):\n",
    "    pipeline_lr_tuned = Pipeline(stages=[assembler, indexer, lr])\n",
    "    paramGrid_lr_tuned = ParamGridBuilder().addGrid(classifier_model_alg.maxIter, maxIter_param).addGrid(classifier_model_alg.regParam, regParam_param).addGrid(classifier_model_alg.elasticNetParam, elasticNetParam_param).build()\n",
    "\n",
    "    # Cross validator for above grid parameters\n",
    "    crossval_lr_tuned = CrossValidator(estimator=classifier_model_alg, estimatorParamMaps=paramGrid_lr_tuned,evaluator=BinaryClassificationEvaluator(),numFolds=3)\n",
    "    crossval_lr_model_tuned = crossval_lr_tuned.fit(train)\n",
    "    prediction_results_tuned = crossval_lr_model_tuned.transform(test)\n",
    "    return prediction_results_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the LogisticRegression alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_lr = tune_model_lr(LogisticRegression(), [10, 20], [0.0, 0.1],[0.0, 0.5],3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7058823529411765\n",
      "F1 Score:0.5841784989858012\n",
      "Test F1 Score : 58.42%\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the LogisticRegression alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr2 = tune_model_lr(LogisticRegression(), [5, 25], [0.1, 0.3],[0.0, 0.8],4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7647058823529411\n",
      "F1 Score:0.7030812324929971\n",
      "Test F1 Score : 70.31%\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7647058823529411\n",
      "F1 Score:0.7030812324929971\n",
      "Test F1 Score : 70.31%\n"
     ]
    }
   ],
   "source": [
    "predictions_lr3 = tune_model_lr(LogisticRegression(), [5, 30], [0.1, 0.5],[0.0, 0.9],4)\n",
    "print_metrics(predictions_lr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the LogisticRegression alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7647058823529411\n",
      "F1 Score:0.7030812324929971\n",
      "Test F1 Score : 70.31%\n"
     ]
    }
   ],
   "source": [
    "predictions_lr4 = tune_model_lr(LogisticRegression(), [10, 60], [0.1, 0.5],[0.0, 1.0],3)\n",
    "print_metrics(predictions_lr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, LogisticRegression was the best model for this work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
